{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Before usage of this API, \n",
    "please ensure the following packages are installed. \n",
    "\n",
    "Tensorflow: 1.11.0\n",
    "Keras: 2.2.4\n",
    "NumPy: 1.15.1\n",
    "\n",
    "Note that you can directly install these packages in ipython notebook\n",
    "through commands like \"!pip install tensorflow==1.11\"\n",
    "'''\n",
    "\n",
    "# Let's start our demestration\n",
    "# For this grid, we import some packages and utils.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras \n",
    "\n",
    "import random, math\n",
    "\n",
    "# You can use the API without creating an utils instance, \n",
    "# We create an utils instance here for printing some information \n",
    "# to illustrate that our operators function correctly \n",
    "import utils\n",
    "utils = utils.GeneralUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_datas shape: (5000, 784)\n",
      "train_labels shape: (5000,)\n",
      "test_datas shape: (1000, 784)\n",
      "test_labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training dataset and untrained model for source-level mutation \n",
    "# Users can their our own dataset and model\n",
    "import network \n",
    "network = network.SimplyNetwork()\n",
    "\n",
    "# model is a simple FC(fully-connected) neural network\n",
    "# dataset is a subset from MNIST dataset with 5000 training data and 1000 testing data\n",
    "model = network.create_debug_model()\n",
    "(train_datas, train_labels), (test_datas, test_labels) = network.load_data()\n",
    "\n",
    "print('train_datas shape:', train_datas.shape)\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print('test_datas shape:', test_datas.shape)\n",
    "print('test_labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of source-level mutation operators API\n",
    "import source_mut_operators\n",
    "source_mut_opts = source_mut_operators.SourceMutationOperators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before DR\n",
      "Train data shape: (5000, 784)\n",
      "Train labels shape: (5000,)\n",
      "\n",
      "After DR, where the mutation ratio is 0.01\n",
      "Train data shape: (5050, 784)\n",
      "Train labels shape: (5050,)\n",
      "\n",
      "Before DR\n",
      "Train data shape: (5000, 784)\n",
      "Train labels shape: (5000,)\n",
      "\n",
      "After DR, where the mutation ratio is 0.1\n",
      "Train data shape: (5500, 784)\n",
      "Train labels shape: (5500,)\n",
      "\n",
      "Before DR\n",
      "Train data shape: (5000, 784)\n",
      "Train labels shape: (5000,)\n",
      "\n",
      "After DR, where the mutation ratio is 0.5\n",
      "Train data shape: (7500, 784)\n",
      "Train labels shape: (7500,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DR (Data Repetition), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "mutation_ratios = [0.01, 0.1, 0.5]\n",
    "for mutation_ratio in mutation_ratios:\n",
    "    \n",
    "    (DR_train_datas, DR_train_labels), DR_model = source_mut_opts.DR_mut((train_datas, train_labels), model, mutation_ratio)\n",
    "    \n",
    "    utils.print_messages_SMO('DR', train_datas=train_datas, train_labels=train_labels, mutated_datas=DR_train_datas, mutated_labels=DR_train_labels, mutation_ratio=mutation_ratio)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation ratio: 0.01\n",
      "Number of mislabeled labels: 50\n",
      "\n",
      "Mutation ratio: 0.1\n",
      "Number of mislabeled labels: 500\n",
      "\n",
      "Mutation ratio: 0.5\n",
      "Number of mislabeled labels: 2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LE (Label Error), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "mutation_ratios = [0.01, 0.1, 0.5]\n",
    "for mutation_ratio in mutation_ratios:\n",
    "    \n",
    "    (LE_train_datas, LE_train_labels), LE_model = source_mut_opts.LE_mut((train_datas, train_labels), model, 0, 9, mutation_ratio)\n",
    "    \n",
    "    mask_equal = LE_train_labels == train_labels\n",
    "    count_diff = len(train_labels) - np.sum(mask_equal)\n",
    "    print('Mutation ratio:', mutation_ratio)\n",
    "    print('Number of mislabeled labels:', count_diff)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before DM\n",
      "Train data shape: (5000, 784)\n",
      "Train labels shape: (5000,)\n",
      "\n",
      "After DM, where the mutation ratio is 0.01\n",
      "Train data shape: (4950, 784)\n",
      "Train labels shape: (4950,)\n",
      "\n",
      "Before DM\n",
      "Train data shape: (5000, 784)\n",
      "Train labels shape: (5000,)\n",
      "\n",
      "After DM, where the mutation ratio is 0.1\n",
      "Train data shape: (4500, 784)\n",
      "Train labels shape: (4500,)\n",
      "\n",
      "Before DM\n",
      "Train data shape: (5000, 784)\n",
      "Train labels shape: (5000,)\n",
      "\n",
      "After DM, where the mutation ratio is 0.5\n",
      "Train data shape: (2500, 784)\n",
      "Train labels shape: (2500,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DM (Data Missing), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "mutation_ratios = [0.01, 0.1, 0.5]\n",
    "for mutation_ratio in mutation_ratios:\n",
    "    \n",
    "    (DM_train_datas, DM_train_labels), DM_model = source_mut_opts.DM_mut((train_datas, train_labels), model, mutation_ratio)\n",
    "    \n",
    "    utils.print_messages_SMO('DM', train_datas=train_datas, train_labels=train_labels, mutated_datas=DM_train_datas, mutated_labels=DM_train_labels, mutation_ratio=mutation_ratio)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For DF, it's a little difficult to explicitly demonstrate\n",
    "a large amount of data samples be shuffled. \n",
    "Here, we simply illustrate how to use DF mutation operator.\n",
    "'''\n",
    "# DF (Data Shuffle), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "mutation_ratio = 0.01\n",
    "(DF_train_datas, DF_train_labels), DF_model = source_mut_opts.DF_mut((train_datas, train_labels), model, mutation_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Mutated data after NP mutation [-0.14919535  1.76744243 -0.18348268 -0.50846262  0.73378591 -0.46038225\n",
      "  0.51430062  1.56414519  0.42217905 -0.19614823  0.30328158  1.16400012\n",
      "  1.17073177 -0.27780963 -0.90304674 -0.47474602  0.07273195 -0.09590933\n",
      "  0.45871262  0.86378239 -0.69907436  1.99007761  0.52903806 -1.0059325\n",
      "  0.44328588  0.30365059  1.57301322 -0.29478877  1.43405793 -0.07993582\n",
      "  1.06541604 -0.25070212 -1.20045648 -1.3176721  -0.05242305  0.39537081\n",
      "  2.23227291  0.36763376  0.51440852 -1.688367    0.94364182 -0.76708495\n",
      " -0.66239884 -0.90091569 -0.17349215  0.11178795  2.08378899 -1.18438874\n",
      "  0.80466717 -0.72205527  0.68004643 -1.41328519 -2.3528229   0.84532262\n",
      " -1.56396574  1.12487683  0.67627296 -1.59416554 -0.09333154  0.6540146\n",
      "  0.84818074  0.94881403  1.69133867 -0.80960796 -0.91923319 -0.8267795\n",
      "  0.30632915 -0.07186162 -1.06725332 -0.57344978  0.3860717   0.05888771\n",
      "  1.41101498 -0.06438508 -0.54061717 -0.7874515   0.17577229  0.65224975\n",
      " -0.83116224 -0.39467321 -0.08216553  0.36705662 -1.25428665 -1.89546552\n",
      "  0.22481654  0.49254115 -1.06442693 -1.10889284 -0.84035773 -0.4399477\n",
      "  1.01548332 -0.65642128 -0.83328363 -0.67044196 -1.90133174  0.33534905\n",
      "  0.64616002 -0.81807865  0.67221167  1.13741864 -0.89128149  0.21765717\n",
      " -1.23831121  0.32756195 -0.7898327  -0.68379792  0.12951533 -0.51998469\n",
      "  1.12854038  0.23542489  1.04949389 -0.72704025  0.37938331 -0.59397602\n",
      "  1.52898642  0.40662492  1.73350112  0.51077951 -0.40642599 -1.44115564\n",
      " -0.1304791   0.31560509 -0.20709047  1.01790937 -0.43840591  0.2834063\n",
      "  1.83207432 -0.72992397 -0.58049464  0.08282568  0.49853244 -0.80113685\n",
      "  0.06761208 -0.74288143  1.04057102  0.9310569  -0.25267457 -1.47343592\n",
      "  0.70556082 -0.23520684  0.27391957  0.60053554 -0.21213318 -0.66469638\n",
      " -0.28547489 -0.76744252 -0.18053231 -0.47819644  0.65210326 -1.9612798\n",
      " -0.43094991 -0.67951235 -1.77392559 -0.37964062  0.53389862  0.33795841\n",
      "  1.20913026 -0.95442555  0.01431747  0.74322628  0.38139826  1.60922715\n",
      "  0.90640053  0.98817459  1.6146066   0.44951047 -0.94287093  1.02017143\n",
      " -1.46959084 -2.86014516 -0.41591823 -2.11019727 -0.5878019  -2.6235606\n",
      "  0.56646577 -0.12447097 -0.78894642  0.91227976 -1.70908719  0.82415502\n",
      "  2.91947611 -1.02905552 -0.74677446  1.199656    1.88722178  1.31177797\n",
      "  0.13552926  2.68418358  1.93236976  0.85623648  1.08704322  0.10478523\n",
      "  2.32916057  1.15594066 -1.64357339  0.12908883 -0.30318031  0.76913903\n",
      " -0.37864973  1.76917396 -0.39886811  1.15452202  0.61012872  0.94959223\n",
      "  1.44369735 -0.15152076  0.31932611 -1.27820155  1.98273598  0.8933513\n",
      " -0.07301324  2.9727756  -0.37489495 -0.28106643  0.78520684  0.97780086\n",
      " -1.19878117  2.89978539  3.11105007  2.05647484 -0.20122677  0.65520967\n",
      " -0.10504959  1.03176109  1.10276544  1.04264401 -2.21140986  1.23767245\n",
      "  1.54505273  0.03033306  0.57503169 -0.99828493  0.52931371  1.61117582\n",
      "  0.2762659   2.58101791  0.6883609   0.6476706  -0.04707475 -0.15041726\n",
      "  0.4123656   1.87443732 -0.48893881  0.17256638  0.88409949  0.44050807\n",
      "  0.56818213  0.16168258  1.82778341 -0.0031536   1.17098998 -0.08548476\n",
      " -0.62179229  0.38630073 -1.48752177 -1.40301448 -0.26124415 -0.79671579\n",
      " -2.52344539 -0.25462542  0.34236423 -0.98849517  1.42906942  1.3189731\n",
      "  1.20753299 -0.39903607  0.86324238 -0.62336052  0.37162372 -0.65638301\n",
      "  0.0504598   0.73721557 -0.32218321 -0.0444562   1.07073415 -0.5629009\n",
      " -0.14081112 -0.39124107 -0.99377033  0.49933019  0.7016852  -0.26401549\n",
      " -0.17648718  0.57851261 -0.24218653 -0.94285184 -1.04967036 -1.45468773\n",
      "  0.16344862 -0.43172675  0.40888237  2.42736868  1.15028878 -0.95076053\n",
      " -0.43068715 -0.57827945 -0.81596059  1.7538867   1.51182477 -0.22005439\n",
      " -0.36457583  1.46180351  0.27800783 -0.28175796  0.82822406  1.28869016\n",
      "  1.65449101  1.06470622  0.75404144  1.85006782 -0.53442523  0.20427783\n",
      "  0.25335028 -0.05903368 -0.91578785  0.6299377  -0.47296445  0.93425299\n",
      "  0.18514749  2.62417067  0.9440903   1.59590765  3.02206705  0.65671191\n",
      "  0.45076154 -1.88376231  0.57973356 -1.91060362  0.87477618  0.92500204\n",
      "  0.75749883  0.46148824  0.52335179 -0.13943382  0.21351258  0.15093186\n",
      "  1.88394202  0.6206181  -0.25567804  1.00849625  0.40608055 -1.64491658\n",
      "  0.63550035  1.05913956  1.46961702  0.07904575 -0.01844561  0.95935442\n",
      "  0.31626046 -0.38845099 -0.22731371 -0.30625382 -1.48568773  0.24438953\n",
      " -1.11346696 -0.59800372  1.06228529  0.64656815  1.47992441 -0.31612177\n",
      "  0.44649225 -1.10750926 -0.83641959 -0.22955749  1.29422363 -0.18051107\n",
      "  1.46828018 -0.75278803 -0.85063109 -0.56697467 -1.21329077  0.83128629\n",
      " -0.576134   -2.18173918 -1.42627709 -0.39762689  0.90088323  0.08474375\n",
      "  1.02566113  0.23674689  0.55190282  2.16330807 -0.78858076 -0.61470431\n",
      "  0.65200313  0.84780493  0.13529191 -0.57525211 -1.18472622  0.31026771\n",
      "  0.15648506 -0.85817664  0.10376398  2.63522136 -1.23605333  0.28297809\n",
      " -0.32659295  1.57041045  1.49286188  0.96190048 -1.37863607  0.15800996\n",
      "  0.65043614 -0.38546592 -0.67862528 -0.66023344  0.24731611  2.21988981\n",
      "  1.99864559 -0.11362684  1.76093284 -0.89045678  1.22251286 -1.44658154\n",
      "  0.75132591  0.48571149  0.28679384 -0.40283543  1.4750534   1.37104968\n",
      "  0.91794421 -0.37239898 -1.81454981 -1.36186771 -0.11314339  0.24723489\n",
      "  1.6781819  -0.09491981 -0.81230453  0.51561447  0.13989853  0.37389323\n",
      " -0.36981647  0.51642492  0.03739737 -0.42833474  0.33049747  0.97583134\n",
      " -0.02592842  0.11637722  1.13385443  2.65062661  1.06953865  0.38138108\n",
      "  0.62778781  0.24908115  0.8799134  -1.39162965  1.54768678  1.24970264\n",
      " -0.29465318 -2.64111511 -0.70280499  1.03199605 -0.40299526 -0.56524464\n",
      "  0.36787486  1.78415264 -0.16507325 -0.52760152 -1.17828178 -0.51573533\n",
      "  0.32995522  0.4374354   1.09381719  1.82395494 -0.24154913  1.00166191\n",
      "  0.33339233 -0.4202004  -0.48229028 -1.28618987  0.9211965   1.78853358\n",
      " -0.70152306  0.0711671   0.24015225 -0.13646302 -0.08546016 -0.68006486\n",
      "  0.56815927 -0.81219659 -0.41755156  1.82321217  0.81246423  0.16167565\n",
      "  0.11492537 -0.60264469 -0.93937686 -0.19021025  0.92480026 -0.48900717\n",
      "  1.94594775  1.38759044  1.47134573  1.21131103 -0.39583269 -0.0439167\n",
      "  1.7817772   2.22495202 -1.58138519 -0.87688002  0.46022502 -1.1428004\n",
      " -0.87141605  0.4263175   0.73528198  1.83480169 -0.30182289  0.92751651\n",
      " -0.41306295 -1.01513182 -2.2108789  -0.78403771 -1.66139715  1.36530192\n",
      "  0.79386277 -0.15992457 -0.31454399 -0.20368861  0.64018423  1.04185389\n",
      "  1.49675684  1.87351461  1.50730676 -2.03566024  0.68668912 -0.70902468\n",
      " -0.55747382 -0.49979132 -2.10319236  0.75649428 -1.34844723 -0.3887227\n",
      " -1.23348556 -0.25686047  1.46283858 -0.46538199 -0.72094643  0.49596914\n",
      " -0.15637417 -0.47047338 -0.32261121 -0.18544636 -0.19053506  1.0724231\n",
      "  1.69606979  1.066297   -0.17045909  1.19261055  0.1195633   0.52905492\n",
      " -1.27536499  1.35687457 -1.16708137 -0.28858215  0.70774047  0.74089092\n",
      " -0.79990099  0.75099706  0.87194919 -0.29388969  0.0493168   0.4391035\n",
      "  0.60491242  1.04648002  2.26536948 -1.73651768 -0.15994837 -0.65564257\n",
      "  0.15974931 -0.31329385  0.33343763  1.01421639  1.27915656  2.27215374\n",
      "  1.64966321 -0.02287432  0.01448518 -1.72595249 -0.55229749 -0.73335608\n",
      " -1.08224673  0.8728666  -0.01119125 -0.1928179   0.32223852 -0.30131658\n",
      " -1.36993758  1.20580953 -2.46120132  0.20848949  0.80919575 -0.43803433\n",
      "  1.32841578  1.11600246 -0.38462189  0.56006751 -0.01667563  1.44596477\n",
      "  2.06859386  0.96048032  1.37241329 -0.57919278 -0.11852984 -1.15826607\n",
      " -1.09258295 -0.36875138  1.43188423 -0.14097567  1.31867952  0.46167744\n",
      "  0.89768453  0.85076133  0.1635886   1.1647359   0.39039425 -3.01023487\n",
      " -0.25129604 -1.67876356 -0.67765837 -0.29433869 -1.24488039  1.70199421\n",
      "  2.89679558  2.39721874  1.93047554  3.01157176  0.54968589  0.64612568\n",
      " -0.74940084 -2.00080331 -0.91055025  1.13753294 -1.02318617  0.88468317\n",
      "  0.6234583  -0.67493103  0.02138517  0.71340217 -0.43333634 -1.86950381\n",
      "  1.57402585  0.73260591 -1.67049225 -1.82082357  2.02531731  0.80667074\n",
      "  0.2587021   1.56187472  0.05055953  1.60122911  1.29456821  1.60485549\n",
      "  0.99445678  0.61704285 -1.12679728  0.00365968  0.62661988 -0.10778777\n",
      "  0.69961144  0.94099795 -1.0246064   1.73403195 -1.80529743  0.82769794\n",
      "  0.62639697 -0.87544493  0.29167031  0.54642119  0.32834301  0.30425298\n",
      " -2.67537568 -1.31962144 -0.99601802  1.41920756  0.51610245  1.15135608\n",
      "  0.40534823  0.26160396  0.61634414  2.22146098  0.45535224  0.70561274\n",
      " -0.62666949 -0.09281165  1.10700504  0.48987948 -0.25750956 -2.24736235\n",
      " -0.04212198  1.34775812 -0.16278273 -0.96675134  0.42323914 -0.02424547\n",
      "  0.44376223  1.20345229 -1.07984013 -1.18738548  1.45060468 -0.42561393\n",
      "  0.45374773  0.66022707  1.5564857  -1.04581224  1.6217875  -0.90193981\n",
      " -0.5363446  -0.36141725 -0.20729241 -1.49836134  0.57078129  0.19911212\n",
      "  1.94196931  0.76811542 -0.11770767  0.9362078  -0.60175184 -0.35197944\n",
      " -0.10414367  0.31621122  0.76174778 -1.18571815 -1.75598894 -1.14998957\n",
      "  0.66032083 -0.54606227 -0.22703961  1.57635453 -0.07118159  0.31459755\n",
      "  1.08274688 -1.76837987 -0.28825426  1.69800679  0.36390759  0.67508451\n",
      "  0.93353531 -0.30987002  1.1944368   1.01018401  1.07097941  0.11192251\n",
      "  1.27780289  1.13852073 -0.25798027 -1.19465473  0.88026605  1.81494271\n",
      "  0.74927534 -1.64253456 -1.03338277  1.46072626 -1.83487936  0.95515666\n",
      " -0.98887871  0.96458888 -0.45886656 -1.20458223 -0.1323326   0.11623599\n",
      "  0.53456325  2.3810305  -0.12844063 -0.9445811  -0.28468083  1.6144641\n",
      " -0.99405876 -0.03717314  0.51540386 -0.79208248 -1.10069964  1.02584219\n",
      "  1.3446438   0.20530223 -2.61981884 -0.3411676   1.188434    2.73077915\n",
      "  1.21117867 -1.36998945 -0.1412844   0.73614321]\n"
     ]
    }
   ],
   "source": [
    "# NP (Noise Perturb), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "mutation_ratio = 1\n",
    "STD = 1\n",
    "(NP_train_datas, NP_train_labels), NP_model = source_mut_opts.NP_mut((train_datas, train_labels), model, mutation_ratio, STD=STD)\n",
    "\n",
    "print('Original data', train_datas[0])\n",
    "print('Mutated data after NP mutation', NP_train_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,994\n",
      "Trainable params: 51,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Before any mutation on model, let's see the architecture of this model. \n",
    "\n",
    "# According to the paper, there is a restriction of layer being added or removed.\n",
    "# The input and output shape of layer being added or removed are required to be same.\n",
    "\n",
    "# Hence, when you look at the architecture of this model. \n",
    "# There are layers with same input and output shape in this model for demenstration purpose.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_copy_LR (Dense)      (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1_copy_LR (Dropout)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2_copy_LR (Dense)      (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_4_copy_LR (Dense)      (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5_copy_LR (Dense)      (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,722\n",
      "Trainable params: 51,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LR (Layer Removal), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "(LR_train_datas, LR_train_labels), LR_model = source_mut_opts.LR_mut((train_datas, train_labels), model)\n",
    "LR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_copy_LAs (Dense)     (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1_copy_LAs (Dropout) (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2_copy_LAs (Dense)     (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_3_copy_LAs (Dense)     (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4_copy_LAs (Dense)     (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5_copy_LAs (Dense)     (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 52,058\n",
      "Trainable params: 52,026\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LAs (Layer Addition for source-level mutation), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "(LAs_train_datas, LAs_train_labels), LAs_model = source_mut_opts.LAs_mut((train_datas, train_labels), model)\n",
    "LAs_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For AFRs, it's a little difficult to explicitly demonstrate\n",
    "Here, we simply illustrate how to use AFRs mutation operator.\n",
    "'''\n",
    "# AFRs (Activation Function Removal for source-level mutation), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "(AFRs_train_datas, AFRs_train_labels), AFRs_model = source_mut_opts.AFRs_mut((train_datas, train_labels), model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
